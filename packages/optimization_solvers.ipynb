{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Julia Workshop: Optimization and Solvers\n",
    "\n",
    "## @ CEF 2017\n",
    "\n",
    "**Authors**: Chase Coleman and Spencer Lyon\n",
    "\n",
    "**Date**: 27 June 2017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "A few different packages used for optimization or non-linear equation solving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuantEcon\n",
    "\n",
    "The QuantEcon library has a few simple optimizers and solvers. See the economics [notebook](economics.ipynb) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pkg.add(\"QuantEcon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optim.jl\n",
    "\n",
    "Package for optimizing written in pure Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pkg.add(\"Optim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rosenbrock (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rosenbrock(x) = (1.0 - x[1])^2 + 100.0*(x[2] - x[1]^2)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing without gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Nelder-Mead\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999710322210338,0.9999438685860869]\n",
       " * Minimum: 1.164323e-09\n",
       " * Iterations: 74\n",
       " * Convergence: true\n",
       "   *  √(Σ(yᵢ-ȳ)²)/n < 1.0e-08: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Function Calls: 108"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(rosenbrock, zeros(2), NelderMead())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: BFGS\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999999926033423,0.9999999852005353]\n",
       " * Minimum: 5.471433e-17\n",
       " * Iterations: 16\n",
       " * Convergence: true\n",
       "   * |x - x'| < 1.0e-32: false\n",
       "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
       "   * |g(x)| < 1.0e-08: true\n",
       "   * f(x) > f(x'): false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Function Calls: 69\n",
       " * Gradient Calls: 69"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(rosenbrock, zeros(2), BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing with gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rosenbrock_grad! (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rosenbrock_grad!(x::Vector, grad::Vector)\n",
    "    grad[1] = -2.0*(1.0 - x[1]) - 400.0*(x[2] - x[1]^2)*x[1]\n",
    "    grad[2] = 200.0*(x[2] - x[1]^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: L-BFGS\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [1.0000000000000007,1.000000000000001]\n",
       " * Minimum: 5.374115e-30\n",
       " * Iterations: 21\n",
       " * Convergence: true\n",
       "   * |x - x'| < 1.0e-10: false\n",
       "   * |f(x) - f(x')| / |f(x)| < 1.0e-09: true\n",
       "   * |g(x)| < 1.0e-08: true\n",
       "   * f(x) > f(x'): false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Function Calls: 90\n",
       " * Gradient Calls: 90"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(rosenbrock, rosenbrock_grad!, zeros(2), LBFGS(),\n",
    "         Optim.Options(x_tol=1e-10, f_tol=1e-9, iterations=25000,\n",
    "                       allow_f_increases=true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Gradient Descent\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999999572646519,0.9999999142802773]\n",
       " * Minimum: 1.832511e-15\n",
       " * Iterations: 18460\n",
       " * Convergence: true\n",
       "   * |x - x'| < 1.0e-10: true\n",
       "   * |f(x) - f(x')| / |f(x)| < 1.0e-09: false\n",
       "   * |g(x)| < 1.0e-08: false\n",
       "   * f(x) > f(x'): false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Function Calls: 64642\n",
       " * Gradient Calls: 64642"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(rosenbrock, rosenbrock_grad!, zeros(2), GradientDescent(),\n",
    "         Optim.Options(x_tol=1e-10, f_tol=1e-9, iterations=25000,\n",
    "                       allow_f_increases=true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing with Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rosenbrock_hess! (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rosenbrock_hess!(x::Vector, hess::Matrix)\n",
    "    hess[1, 1] = 2.0 - 400.0 * x[2] + 1200.0*x[1]^2\n",
    "    hess[1, 2] = -400.0 * x[1]\n",
    "    hess[2, 1] = -400.0 * x[1]\n",
    "    hess[2, 2] = 200.0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Newton's Method\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999999999999994,0.9999999999999989]\n",
       " * Minimum: 3.081488e-31\n",
       " * Iterations: 14\n",
       " * Convergence: true\n",
       "   * |x - x'| < 1.0e-32: false\n",
       "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
       "   * |g(x)| < 1.0e-08: true\n",
       "   * f(x) > f(x'): false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Function Calls: 58\n",
       " * Gradient Calls: 58"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(rosenbrock, rosenbrock_grad!, rosenbrock_hess!, zeros(2), Newton())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLopt\n",
    "\n",
    "Julia wrapper to high quality C library.\n",
    "\n",
    "This library has _lots_ of options for algorithms. See [list](http://ab-initio.mit.edu/wiki/index.php/NLopt_Algorithms) of algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pkg.add(\"NLopt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using NLopt.optimize in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "using NLopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rosenbrock_nlopt (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rosenbrock_nlopt(x::Vector, grad::Vector)\n",
    "    if length(grad) > 0\n",
    "        rosenbrock_grad!(x, grad)\n",
    "    end\n",
    "\n",
    "    return rosenbrock(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A non-gradient and gradient based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.251384063527492e-21,[1.0,1.0],:SUCCESS)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_LBFGS = Opt(:LD_LBFGS, 2)\n",
    "\n",
    "min_objective!(opt_LBFGS, rosenbrock_nlopt)\n",
    "xtol_rel!(opt_LBFGS, 1e-10)\n",
    "ftol_rel!(opt_LBFGS, 1e-9)\n",
    "\n",
    "NLopt.optimize(opt_LBFGS, zeros(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5954711808094964e-28,[1.0,1.0],:SUCCESS)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_PRAXIS = Opt(:LN_PRAXIS, 2)\n",
    "\n",
    "min_objective!(opt_PRAXIS, rosenbrock_nlopt)\n",
    "\n",
    "NLopt.optimize(opt_PRAXIS, zeros(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A constrained optimization problem\n",
    "\n",
    "\\begin{align*}\n",
    "  \\min_{x_1, x_2} &\\sqrt{x_2} \\\\\n",
    "  &\\text{subject to } \\\\\n",
    "  &x_2 \\geq 0 \\\\\n",
    "  &x_2 \\geq (2 x_1)^3  \\\\\n",
    "  &x_2 \\geq (-x_1 + 1)^3\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myconstraint (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function myfunc(x::Vector, grad::Vector)\n",
    "    if length(grad) > 0\n",
    "        grad[1] = 0\n",
    "        grad[2] = 0.5/sqrt(x[2])\n",
    "    end\n",
    "\n",
    "    return sqrt(x[1] + x[2])\n",
    "end\n",
    "\n",
    "function myconstraint(x::Vector, grad::Vector, a, b)\n",
    "    if length(grad) > 0\n",
    "        grad[1] = 3a * (a*x[1] + b)^2\n",
    "        grad[2] = -1\n",
    "    end\n",
    "\n",
    "    return (a*x[1] + b)^3 - x[2]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 0.7934920514599207 at [0.333333,0.296296] (returned XTOL_REACHED)\n"
     ]
    }
   ],
   "source": [
    "opt = Opt(:LD_MMA, 2)\n",
    "\n",
    "lower_bounds!(opt, [-Inf, 0.])\n",
    "xtol_rel!(opt, 1e-6)\n",
    "\n",
    "min_objective!(opt, myfunc)\n",
    "\n",
    "inequality_constraint!(opt, (x,g) -> myconstraint(x, g, 2.0, 0.0), 1e-8)\n",
    "inequality_constraint!(opt, (x,g) -> myconstraint(x, g, -1.0 ,1.0), 1e-8)\n",
    "\n",
    "(minf, minx, ret) = NLopt.optimize(opt, [1.234, 5.678])\n",
    "\n",
    "println(\"got $minf at $minx (returned $ret)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NLsolve\n",
    "\n",
    "Julia package written to solve systems of non-linear equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pkg.add(\"NLsolve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Nonlinear Solver Algorithm\n",
       " * Algorithm: Trust-region with dogleg and autoscaling\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Zero: [0.0,0.0]\n",
       " * Inf-norm of residuals: 1.000000\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| < 0.0e+00: false\n",
       "   * |f(x)| < 1.0e-10: false\n",
       " * Function Calls (f): 1001\n",
       " * Jacobian Calls (df/dx): 1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function f!(xy::Vector, fxy::Vector)\n",
    "    # Pull out arguments\n",
    "    x, y = xy\n",
    "\n",
    "    # Fill fxy\n",
    "    fxy[1] = x^2 - sin(y)\n",
    "    fxy[2] = y^2 - cos(x)\n",
    "end\n",
    "\n",
    "function g!(xy::Vector, jacxy::Matrix)\n",
    "    x, y = xy\n",
    "    # Fill with derivatives of first function\n",
    "    jacxy[1, 1] = 2*x\n",
    "    jacxy[1, 2] = -cos(y)\n",
    "\n",
    "    # Fill off-diagonal\n",
    "    jacxy[2, 1] = sin(x)\n",
    "    jacxy[2, 2] = 2*y\n",
    "end\n",
    "\n",
    "res = nlsolve(f!, g!, [0.0, 0.0], ftol=1e-10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
